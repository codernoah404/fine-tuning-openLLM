{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "## 데이터 관련 라이브러리 로드 \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "\n",
    "\n",
    "## LLM, 딥러닝  관련 라이브러리 로드 \n",
    "\n",
    "import torch \n",
    "\n",
    "from transformers import AutoTokenizer #토크나이저 \n",
    "from transformers import LlamaForCausalLM,  AutoModelForCausalLM\n",
    " # LLM 모델 \n",
    "from transformers import BitsAndBytesConfig # 양자화 라이브러리 \n",
    "from transformers import GenerationConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training # 효율적 학습을 위한 라이브러리 , LORA 관련 라이브러리 \n",
    "from transformers import Trainer, TrainingArguments # 학습 관련된 모델 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset : maywell/ko_wikidata_QA\n",
    "dataset = load_dataset('maywell/ko_wikidata_QA')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"beomi/llama-2-ko-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, padding_side = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt = \"\"\" ###\n",
    "\n",
    "### %s \n",
    "\n",
    "### %s \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(element):\n",
    "    return DatasetDict({'tmp_promt': dataset_prompt%(element['instruction'], element['output'])})\n",
    "\n",
    "\n",
    "dataset['train'] = dataset['train'].map(gen_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    \n",
    "    outputs = tokenizer(\n",
    "        element['tmp_promt'],\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset['train'].map(\n",
    "    tokenize, batched=True, remove_columns=dataset['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size = 0.2, shuffle =True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for mps device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    \n",
    "mps_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4bit quantaziation (양자화 라이브러리가 m1 실리콘에는 지원하지 않기 때문에 모델을 따로 다운로드하여 양자화 진행한 후 mps 환경에서 훈련합니다.)\n",
    "\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "use_4bit = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.bfloat16 and use_4bit:\n",
    "    major = torch.mps.driver_allocated_memory()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply quantization on model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef563e856e14668b2dab8d2a6d2a8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 4096}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LlamaForCausalLM.from_pretrained(base_model)\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) #freeze\n",
    "\n",
    "output_dir = \"./prac/output\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "torch.save({}, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/inhwancho/Desktop/dino_ai/dino_2nd_LLM/training\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../llama.cpp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file ../dino_2nd_LLM/training/prac/output/model-00001-of-00006.safetensors\n",
      "Loading model file ../dino_2nd_LLM/training/prac/output/model-00001-of-00006.safetensors\n",
      "Loading model file ../dino_2nd_LLM/training/prac/output/model-00002-of-00006.safetensors\n",
      "Loading model file ../dino_2nd_LLM/training/prac/output/model-00003-of-00006.safetensors\n",
      "Loading model file ../dino_2nd_LLM/training/prac/output/model-00004-of-00006.safetensors\n",
      "Loading model file ../dino_2nd_LLM/training/prac/output/model-00005-of-00006.safetensors\n",
      "Loading model file ../dino_2nd_LLM/training/prac/output/model-00006-of-00006.safetensors\n",
      "params = Params(n_vocab=46336, n_embd=4096, n_layer=32, n_ctx=2048, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../dino_2nd_LLM/training/prac/output'))\n",
      "Loaded vocab file PosixPath('../dino_2nd_LLM/training/prac/output/tokenizer.json'), type 'hfft'\n",
      "Vocab info: <LlamaHfVocab with 46336 base tokens and 0 added tokens>\n",
      "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 2}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F32    | [46336, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F32    | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F32    | [4096, 11008]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F32    | [11008, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F32    | [11008, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F32    | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F32    | [4096, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F32    | [4096, 4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F32    | [4096, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | F32    | [46336, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F32    | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F32    | [4096, 11008]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F32    | [11008, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F32    | [11008, 4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F32    | [4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F32    | [4096, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F32    | [4096, 4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | F32    | [4096]\n",
      "Writing ../dino_2nd_LLM/training/prac/output/ggml-model-f32.gguf, format 0\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Setting special token type bos to 1\n",
      "gguf: Setting special token type eos to 2\n",
      "gguf: Setting special token type unk to 0\n",
      "gguf: Setting special token type pad to 2\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/291] Writing tensor token_embd.weight                      | size  46336 x   4096  | type F32  | T+   2\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+   2\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+   2\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+   2\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F32  | T+   3\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F32  | T+   3\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F32  | T+   3\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F32  | T+   3\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+   4\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+   4\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+   4\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F32  | T+   4\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F32  | T+   4\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F32  | T+   4\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F32  | T+   4\n",
      "[ 20/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[ 21/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+   6\n",
      "[ 22/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+   6\n",
      "[ 23/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+   6\n",
      "[ 24/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   6\n",
      "[ 25/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F32  | T+   6\n",
      "[ 26/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F32  | T+   6\n",
      "[ 27/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F32  | T+   6\n",
      "[ 28/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F32  | T+   6\n",
      "[ 29/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[ 30/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+   7\n",
      "[ 31/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+   8\n",
      "[ 32/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+   8\n",
      "[ 33/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   8\n",
      "[ 34/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F32  | T+   8\n",
      "[ 35/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F32  | T+   8\n",
      "[ 36/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F32  | T+   8\n",
      "[ 37/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F32  | T+   8\n",
      "[ 38/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[ 39/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+   9\n",
      "[ 40/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+   9\n",
      "[ 41/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+   9\n",
      "[ 42/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  10\n",
      "[ 43/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F32  | T+  10\n",
      "[ 44/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F32  | T+  10\n",
      "[ 45/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  10\n",
      "[ 46/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F32  | T+  10\n",
      "[ 47/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F32  | T+  10\n",
      "[ 48/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  10\n",
      "[ 49/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  10\n",
      "[ 50/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  11\n",
      "[ 51/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  11\n",
      "[ 52/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  11\n",
      "[ 53/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  11\n",
      "[ 54/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  11\n",
      "[ 55/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F32  | T+  11\n",
      "[ 56/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  11\n",
      "[ 57/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  12\n",
      "[ 58/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  12\n",
      "[ 59/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F32  | T+  12\n",
      "[ 60/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  12\n",
      "[ 61/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  12\n",
      "[ 62/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[ 63/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+  13\n",
      "[ 64/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+  13\n",
      "[ 65/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+  14\n",
      "[ 66/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  14\n",
      "[ 67/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F32  | T+  14\n",
      "[ 68/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F32  | T+  14\n",
      "[ 69/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[ 70/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+  14\n",
      "[ 71/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+  15\n",
      "[ 72/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+  15\n",
      "[ 73/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  15\n",
      "[ 74/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F32  | T+  15\n",
      "[ 75/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F32  | T+  15\n",
      "[ 76/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  15\n",
      "[ 77/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F32  | T+  15\n",
      "[ 78/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  15\n",
      "[ 79/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+  16\n",
      "[ 80/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+  16\n",
      "[ 81/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+  16\n",
      "[ 82/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  16\n",
      "[ 83/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F32  | T+  16\n",
      "[ 84/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F32  | T+  16\n",
      "[ 85/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  17\n",
      "[ 86/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F32  | T+  17\n",
      "[ 87/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  17\n",
      "[ 88/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+  18\n",
      "[ 89/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+  18\n",
      "[ 90/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+  18\n",
      "[ 91/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  18\n",
      "[ 92/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F32  | T+  18\n",
      "[ 93/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F32  | T+  18\n",
      "[ 94/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  18\n",
      "[ 95/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F32  | T+  18\n",
      "[ 96/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[ 97/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F32  | T+  19\n",
      "[ 98/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F32  | T+  19\n",
      "[ 99/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F32  | T+  20\n",
      "[100/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  20\n",
      "[101/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F32  | T+  20\n",
      "[102/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F32  | T+  20\n",
      "[103/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  20\n",
      "[104/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F32  | T+  20\n",
      "[105/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  20\n",
      "[106/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  21\n",
      "[107/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  21\n",
      "[108/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  22\n",
      "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  22\n",
      "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  22\n",
      "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  22\n",
      "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  22\n",
      "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  22\n",
      "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  22\n",
      "[115/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  22\n",
      "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F32  | T+  22\n",
      "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  23\n",
      "[118/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  23\n",
      "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  23\n",
      "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  24\n",
      "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  24\n",
      "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  24\n",
      "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  24\n",
      "[124/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  24\n",
      "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F32  | T+  24\n",
      "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  24\n",
      "[127/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  24\n",
      "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  24\n",
      "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  25\n",
      "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  25\n",
      "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  26\n",
      "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  26\n",
      "[133/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  26\n",
      "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F32  | T+  26\n",
      "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  26\n",
      "[136/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  26\n",
      "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  26\n",
      "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  27\n",
      "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  27\n",
      "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  27\n",
      "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  27\n",
      "[142/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  27\n",
      "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F32  | T+  27\n",
      "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  27\n",
      "[145/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  27\n",
      "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  27\n",
      "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  29\n",
      "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  29\n",
      "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  29\n",
      "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[151/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  29\n",
      "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F32  | T+  29\n",
      "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  29\n",
      "[154/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  29\n",
      "[155/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  30\n",
      "[156/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F32  | T+  30\n",
      "[157/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  30\n",
      "[158/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  30\n",
      "[159/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  30\n",
      "[160/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  31\n",
      "[161/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  31\n",
      "[162/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  32\n",
      "[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  32\n",
      "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  32\n",
      "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  33\n",
      "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  33\n",
      "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  33\n",
      "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  33\n",
      "[169/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  33\n",
      "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F32  | T+  33\n",
      "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  34\n",
      "[172/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  34\n",
      "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  34\n",
      "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  35\n",
      "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  35\n",
      "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  35\n",
      "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  35\n",
      "[178/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  35\n",
      "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F32  | T+  35\n",
      "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  35\n",
      "[181/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  35\n",
      "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  35\n",
      "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  38\n",
      "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  38\n",
      "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  38\n",
      "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  38\n",
      "[187/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  38\n",
      "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F32  | T+  38\n",
      "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  38\n",
      "[190/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  38\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  38\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  39\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  39\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  40\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  40\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  40\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F32  | T+  40\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  40\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  40\n",
      "[200/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  40\n",
      "[201/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  41\n",
      "[202/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  42\n",
      "[203/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  42\n",
      "[204/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  42\n",
      "[205/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  42\n",
      "[206/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F32  | T+  42\n",
      "[207/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  42\n",
      "[208/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  42\n",
      "[209/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  43\n",
      "[210/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F32  | T+  43\n",
      "[211/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  43\n",
      "[212/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  43\n",
      "[213/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
      "[214/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  44\n",
      "[215/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  44\n",
      "[216/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  44\n",
      "[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  45\n",
      "[218/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  45\n",
      "[219/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  45\n",
      "[220/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  45\n",
      "[221/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  46\n",
      "[222/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  46\n",
      "[223/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  46\n",
      "[224/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F32  | T+  46\n",
      "[225/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  46\n",
      "[226/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  46\n",
      "[227/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  46\n",
      "[228/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  47\n",
      "[229/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  47\n",
      "[230/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  48\n",
      "[231/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  48\n",
      "[232/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  48\n",
      "[233/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F32  | T+  48\n",
      "[234/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  48\n",
      "[235/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  48\n",
      "[236/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  48\n",
      "[237/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  49\n",
      "[238/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  49\n",
      "[239/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  49\n",
      "[240/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  50\n",
      "[241/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  50\n",
      "[242/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F32  | T+  50\n",
      "[243/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  50\n",
      "[244/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  50\n",
      "[245/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  50\n",
      "[246/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  51\n",
      "[247/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  51\n",
      "[248/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  51\n",
      "[249/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  52\n",
      "[250/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  52\n",
      "[251/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F32  | T+  52\n",
      "[252/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  52\n",
      "[253/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  52\n",
      "[254/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  52\n",
      "[255/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  53\n",
      "[256/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  54\n",
      "[257/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  54\n",
      "[258/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  54\n",
      "[259/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  54\n",
      "[260/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F32  | T+  54\n",
      "[261/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  54\n",
      "[262/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  54\n",
      "[263/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  55\n",
      "[264/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F32  | T+  55\n",
      "[265/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  55\n",
      "[266/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  55\n",
      "[267/291] Writing tensor output.weight                          | size  46336 x   4096  | type F32  | T+  58\n",
      "[268/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  58\n",
      "[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  58\n",
      "[270/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  58\n",
      "[271/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  59\n",
      "[272/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  59\n",
      "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  59\n",
      "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  59\n",
      "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  60\n",
      "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  60\n",
      "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  60\n",
      "[278/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  60\n",
      "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F32  | T+  60\n",
      "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  60\n",
      "[281/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  60\n",
      "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  60\n",
      "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F32  | T+  61\n",
      "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F32  | T+  62\n",
      "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F32  | T+  62\n",
      "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  62\n",
      "[287/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F32  | T+  62\n",
      "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F32  | T+  62\n",
      "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  62\n",
      "[290/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F32  | T+  62\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  62\n",
      "Wrote ../dino_2nd_LLM/training/prac/output/ggml-model-f32.gguf\n"
     ]
    }
   ],
   "source": [
    "!python convert.py ../dino_2nd_LLM/training/prac/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 2616 (75cd4c77)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.4.0\n",
      "main: quantizing '../dino_2nd_LLM/training/prac/output/ggml-model-f32.gguf' to '../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.bin' as Q4_0\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ../dino_2nd_LLM/training/prac/output/ggml-model-f32.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = prac\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 46336\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 0\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,46336]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,46336]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,46336]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:  291 tensors\n",
      "llama_model_quantize_internal: meta size = 1093280 bytes\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 46336,     1,     1], type =    f32, converting to q4_0 .. size =   724.00 MiB ->   101.81 MiB\n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  47/ 291]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  48/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  49/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  50/ 291]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  51/ 291]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  52/ 291]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  53/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  54/ 291]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  55/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  56/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  57/ 291]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  58/ 291]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  59/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  60/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  61/ 291]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  62/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  63/ 291]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  64/ 291]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  65/ 291]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  66/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  68/ 291]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  69/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  70/ 291]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  71/ 291]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  72/ 291]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  73/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  74/ 291]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  75/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  76/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  77/ 291]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  78/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  79/ 291]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  80/ 291]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  81/ 291]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  82/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  83/ 291]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  84/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  85/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  86/ 291]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  87/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 291]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  89/ 291]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  90/ 291]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  91/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  92/ 291]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  93/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  94/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  95/ 291]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[  96/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  97/ 291]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  98/ 291]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[  99/ 291]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 100/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 101/ 291]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 102/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 103/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 104/ 291]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 105/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 291]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 107/ 291]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 108/ 291]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 137/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 139/ 291]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 140/ 291]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 141/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 143/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 144/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 145/ 291]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 155/ 291]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 156/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 157/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 158/ 291]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 159/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 291]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 161/ 291]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 162/ 291]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 174/ 291]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 209/ 291]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 210/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 211/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 212/ 291]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 213/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 214/ 291]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 215/ 291]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 216/ 291]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 263/ 291]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 264/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 265/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 266/ 291]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 267/ 291]                        output.weight - [ 4096, 46336,     1,     1], type =    f32, converting to q6_K .. size =   724.00 MiB ->   148.48 MiB\n",
      "[ 268/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 269/ 291]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 270/ 291]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 271/ 291]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 274/ 291]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f32, converting to q4_0 .. size =   172.00 MiB ->    24.19 MiB\n",
      "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_0 .. size =    64.00 MiB ->     9.00 MiB\n",
      "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 26153.02 MB\n",
      "llama_model_quantize_internal: quant size  =  3725.30 MB\n",
      "\n",
      "main: quantize time = 10698.81 ms\n",
      "main:    total time = 10698.81 ms\n"
     ]
    }
   ],
   "source": [
    "!./quantize ../dino_2nd_LLM/training/prac/output/ggml-model-f32.gguf ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/inhwancho/Desktop/dino_ai/dino_2nd_LLM/training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./prac/output/ggml-model-f32_q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = prac\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 46336\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,46336]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,46336]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,46336]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 266/46336 vs 259/46336 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 46336\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.86 B\n",
      "llm_load_print_meta: model size       = 3.64 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = prac\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   108.61 MiB, (  108.67 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3725.30 MiB\n",
      "llm_load_tensors:      Metal buffer size =   108.60 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, (  142.48 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.18 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   164.02 MiB, (  306.50 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '2048', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'prac', 'llama.vocab_size': '46336'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"./prac/output/ggml-model-f32_q4_0.gguf\"\n",
    "model = Llama(model_path = model_path,\n",
    "              n_ctx = 2048,            # context window size\n",
    "              n_gpu_layers = 1,        # enable GPU\n",
    "              use_mlock = True)        # enable memory lock so not swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please specify `target_modules` in `peft_config`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCAUSAL_LM,\n\u001b[1;32m      2\u001b[0m                         inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# 학습하는지  \u001b[39;00m\n\u001b[1;32m      3\u001b[0m                         r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, \u001b[38;5;66;03m# 작을 수록 trainable 한 파라미터의 개수가 낮아진다\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                         lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# scaling factor \u001b[39;00m\n\u001b[1;32m      5\u001b[0m                         lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;66;03m# dropout \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_tune/lib/python3.8/site-packages/peft/mapping.py:136\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    135\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_tune/lib/python3.8/site-packages/peft/peft_model.py:1094\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_tune/lib/python3.8/site-packages/peft/peft_model.py:129\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_tune/lib/python3.8/site-packages/peft/tuners/lora/model.py:136\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_tune/lib/python3.8/site-packages/peft/tuners/tuners_utils.py:148\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mupdate(peft_config)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_tune/lib/python3.8/site-packages/peft/tuners/tuners_utils.py:293\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m--> 293\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_adapter_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model(peft_config, model)\n\u001b[1;32m    296\u001b[0m is_target_modules_in_base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_tune/lib/python3.8/site-packages/peft/tuners/lora/model.py:412\u001b[0m, in \u001b[0;36mLoraModel._prepare_adapter_config\u001b[0;34m(peft_config, model_config)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mtarget_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:\n\u001b[0;32m--> 412\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify `target_modules` in `peft_config`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    413\u001b[0m     peft_config\u001b[38;5;241m.\u001b[39mtarget_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    414\u001b[0m         TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    415\u001b[0m     )\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m peft_config\n",
      "\u001b[0;31mValueError\u001b[0m: Please specify `target_modules` in `peft_config`"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                        inference_mode=False, # 학습하는지  \n",
    "                        r=16, # 작을 수록 trainable 한 파라미터의 개수가 낮아진다\n",
    "                        lora_alpha=16,  # scaling factor \n",
    "                        lora_dropout=0.1) # dropout \n",
    "\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "###당신은 친절하고 정직한 인공지능 비서입니다. 당신은 항상 유용하고 안전한 답변을 기용하고 유해하거나, 비윤리적이거나, 인종차별적이거나, 성차별적이거나, 위험하거나 불법적인 답변이 포함되어서는 안 됩니다. 당신의 응답은 사회적으로 편견이 없고 긍정적인 내용이어야 합니다. 질문이 의미가 없거나 사실적으로 일관성이 없다면, 옳지 않은 것에 대답하는 대신 이유를 설명하십시오. 질문에 대한 답을 모르는 경우 허위 정보를 공유하지 마십시오.\n",
    "\n",
    "코딩 공부를 하는 법을 알려줘\n",
    "\"\"\"\n",
    "\n",
    "output = model(prompt = prompt, max_tokens = 120, temperature = 0.2)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max length: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "# Change the max length depending on hardware constraints.\n",
    "max_length = get_max_length(model)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"common_sense_llama\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=3000,\n",
    "    logging_steps=1000,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    optim = \"adamw_torch\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 300,\n",
    "    save_total_limit=2\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"mps\",\n",
    "    device=1,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기부터 위로 올릴거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
