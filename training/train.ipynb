{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "## 데이터 관련 라이브러리 로드 \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "\n",
    "\n",
    "## LLM, 딥러닝  관련 라이브러리 로드 \n",
    "\n",
    "import torch \n",
    "\n",
    "from transformers import AutoTokenizer #토크나이저 \n",
    "from transformers import LlamaForCausalLM,  AutoModelForCausalLM\n",
    " # LLM 모델 \n",
    "from transformers import BitsAndBytesConfig # 양자화 라이브러리 \n",
    "from transformers import GenerationConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training # 효율적 학습을 위한 라이브러리 , LORA 관련 라이브러리 \n",
    "from transformers import Trainer, TrainingArguments # 학습 관련된 모델 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 137505\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset : maywell/ko_wikidata_QA\n",
    "dataset = load_dataset('maywell/ko_wikidata_QA')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"beomi/llama-2-ko-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, padding_side = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt = \"\"\" ###\n",
    "\n",
    "### %s \n",
    "\n",
    "### %s \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(element):\n",
    "    return DatasetDict({'tmp_promt': dataset_prompt%(element['instruction'], element['output'])})\n",
    "\n",
    "\n",
    "dataset['train'] = dataset['train'].map(gen_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [promt['tmp_promt'] for promt in dataset['train']]\n",
    "with open('../Dataset/data.txt', 'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for mps device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    \n",
    "mps_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4bit quantaziation (양자화 라이브러리가 m1 실리콘에는 지원하지 않기 때문에 모델을 따로 다운로드하여 양자화 진행한 후 mps 환경에서 훈련합니다.)\n",
    "\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "use_4bit = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.bfloat16 and use_4bit:\n",
    "    major = torch.mps.driver_allocated_memory()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply quantization on model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LlamaForCausalLM.from_pretrained(base_model)\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) #freeze\n",
    "\n",
    "output_dir = \"./prac/output\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "torch.save({}, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/inhwancho/Desktop/dino_ai/dino_2nd_LLM/training\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../llama.cpp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert.py ../dino_2nd_LLM/training/prac/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./quantize ../dino_2nd_LLM/training/prac/output/ggml-model-f32.gguf ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf Q4_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract a LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                        inference_mode=False, # 학습하는지  \n",
    "                        r=16, # 작을 수록 trainable 한 파라미터의 개수가 낮아진다\n",
    "                        lora_alpha=16,  # scaling factor \n",
    "                        lora_dropout=0.1) # dropout \n",
    "\n",
    "model = get_peft_model(model, peft_config) #불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./prac/LoRA\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "torch.save({}, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.0.self_attn.q_proj => blk.0.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.0.self_attn.v_proj => blk.0.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.1.self_attn.q_proj => blk.1.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.1.self_attn.v_proj => blk.1.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.10.self_attn.q_proj => blk.10.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.10.self_attn.v_proj => blk.10.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.11.self_attn.q_proj => blk.11.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.11.self_attn.v_proj => blk.11.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.12.self_attn.q_proj => blk.12.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.12.self_attn.v_proj => blk.12.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.13.self_attn.q_proj => blk.13.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.13.self_attn.v_proj => blk.13.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.14.self_attn.q_proj => blk.14.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.14.self_attn.v_proj => blk.14.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.15.self_attn.q_proj => blk.15.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.15.self_attn.v_proj => blk.15.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.16.self_attn.q_proj => blk.16.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.16.self_attn.v_proj => blk.16.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.17.self_attn.q_proj => blk.17.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.17.self_attn.v_proj => blk.17.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.18.self_attn.q_proj => blk.18.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.18.self_attn.v_proj => blk.18.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.19.self_attn.q_proj => blk.19.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.19.self_attn.v_proj => blk.19.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.2.self_attn.q_proj => blk.2.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.2.self_attn.v_proj => blk.2.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.20.self_attn.q_proj => blk.20.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.20.self_attn.v_proj => blk.20.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.21.self_attn.q_proj => blk.21.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.21.self_attn.v_proj => blk.21.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.22.self_attn.q_proj => blk.22.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.22.self_attn.v_proj => blk.22.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.23.self_attn.q_proj => blk.23.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.23.self_attn.v_proj => blk.23.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.24.self_attn.q_proj => blk.24.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.24.self_attn.v_proj => blk.24.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.25.self_attn.q_proj => blk.25.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.25.self_attn.v_proj => blk.25.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.26.self_attn.q_proj => blk.26.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.26.self_attn.v_proj => blk.26.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.27.self_attn.q_proj => blk.27.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.27.self_attn.v_proj => blk.27.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.28.self_attn.q_proj => blk.28.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.28.self_attn.v_proj => blk.28.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.29.self_attn.q_proj => blk.29.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.29.self_attn.v_proj => blk.29.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.3.self_attn.q_proj => blk.3.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.3.self_attn.v_proj => blk.3.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.30.self_attn.q_proj => blk.30.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.30.self_attn.v_proj => blk.30.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.31.self_attn.q_proj => blk.31.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.31.self_attn.v_proj => blk.31.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.4.self_attn.q_proj => blk.4.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.4.self_attn.v_proj => blk.4.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.5.self_attn.q_proj => blk.5.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.5.self_attn.v_proj => blk.5.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.6.self_attn.q_proj => blk.6.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.6.self_attn.v_proj => blk.6.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.7.self_attn.q_proj => blk.7.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.7.self_attn.v_proj => blk.7.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.8.self_attn.q_proj => blk.8.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.8.self_attn.v_proj => blk.8.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.9.self_attn.q_proj => blk.9.attn_q.weight.loraB (4096, 16) float32 0.25MB\n",
      "model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraA (4096, 16) float32 0.25MB\n",
      "model.layers.9.self_attn.v_proj => blk.9.attn_v.weight.loraB (4096, 16) float32 0.25MB\n",
      "Converted ../dino_2nd_LLM/training/prac/LoRA/adapter_config.json and ../dino_2nd_LLM/training/prac/LoRA/adapter_model.safetensors to ../dino_2nd_LLM/training/prac/LoRA/ggml-adapter-model.bin\n"
     ]
    }
   ],
   "source": [
    "!python convert-lora-to-ggml.py ../dino_2nd_LLM/training/prac/LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2616 (75cd4c77)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.4.0\n",
      "main: seed  = 1712591948\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = prac\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 46336\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,46336]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,46336]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,46336]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 266/46336 vs 259/46336 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 46336\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.86 B\n",
      "llm_load_print_meta: model size       = 3.64 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = prac\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  3623.50 MiB, ( 3623.56 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  3623.49 MiB\n",
      "llm_load_tensors:        CPU buffer size =   101.81 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/inhwancho/Desktop/dino_ai/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 3881.38 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.18 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    98.50 MiB, ( 3979.88 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =    98.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_apply_lora_from_file_internal: applying lora adapter from '../dino_2nd_LLM/training/prac/LoRA/ggml-adapter-model.bin' - please wait ...\n",
      "llama_apply_lora_from_file_internal: r = 16, alpha = 16, scaling = 1.00\n",
      "llama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base\n",
      "................ done (1195.34 ms)\n",
      "\n",
      "system_info: n_threads = 6 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "\n",
      " 관계부처 합동 태스크포스(TF)팀도 13일 \"내일 10시 2차 TF팀 회의를 열어 향후 대책을 논의할 것\"이라고 밝혔다. 정부는 TF팀 회의에서 1차 TF팀 논의 때와 마찬가지로 부동산시장 동향과 최근 금리인상 및 주택금융공사 보증한도 축소를 둘러싼 시장의 반응 등에 대해 논의할 방침이다. 1차 TF팀 회의에는 국토부, 기획재정부, 금융위 등 정부 부처와 금융기관, 학계, 연구기관 관계자 등 20여명이 참석했다. 정부는 오는 15일 2차 TF팀 회의를 열고 이 자리에서 대책을 최종 확정한다는 방침이다. 한편 이날 서울 강남구 대치동 은마아파트의 재건축추진위원회가 전체 주민을 대상으로 11월 15~20일 6일간의 임시총회를 개최하기로 결정하고 다음달 1일 관련 내용을 관할 구청에 신고할 것으로 전해졌다. 앞서 은마아파트 재건축추진위원회는 지난달 30일 임시총회를 열어 \"11월 15일~20일 6일간 임시총회를 열어 정비계획안 등 재건축 추진을 위한 주요 안건을 상정, 처리하기로 결정했다\"고 밝혔다. 추진위는 \"일부 주민이 재건축 추진에 대한 강한 반대의견을 제시하고 있지만 총회를 통해 추진위가 정식으로 승인되면 모든 권한을 위임받은 조합을 설립해 재건축에 박차를 가할 수 있다\"고 설명했다. 이에 따라 11월 15~20일 6일간의 임시총회에서 재건축을 반대하는 주민과 찬성하는 주민의 찬반 투표 결과가 나올 가능성이 크다. 서울시와 은마아파트 재건축추진위원회는 지난해 5월 은마아파트의 재건축 추진을 위해 관련 규정을 개정한 이후 첫 총회를 개최한다. 1979년 7월 입주한 은마아파트는 30년 가까이 노후화가 진행돼 재건축을 추진해야 한다는 주민들의 목소리가 높았다. 은마아파트의 총회는 2008년 10월 이후 7년 만에 열린다. 은마아파트의 총회 개최로 강남권 재건축 사업이 탄력을 받을 것이란 기대감이 커지고 있다.<|endoftext|>정이품송에서 내려다 본 전경(위)과 정이품송을 위에서 내려다 본 전경(아래)1. 여행개요 �� 일 시 : 2015.05.02(토)�� 장 소 : 충청북도 청주시 일원��� 함께한 이 : 이석영, 김영진, 정두영, 이효정, 유정재, 윤기환, 조동현, 장승호�� 여행시간 : 약 1시간30분 2. 여행지 설명 1) 정이품송일제강점기에 이 나무의 가지를 꺽어 조선총독부의 뜰 앞에 꽂고, 나무에 '이왕가목(李王家木)'이라는 팻말을 붙였다고 한다. 그 팻말을 떼어낸 뒤 1960년대부터 '정이품송'으로 부르고 있다. 2) 청주시 문의면소재지 문의문화재단지1993년 문산관, 대청관, 청백당, 장판각, 망선루 등 충청북도 유형문화재와 문의현의 객사인 대청관이 이전 복원되었다. 이곳은 1980년 대청댐 준공으로 수몰된 지역 내에 있던 문의향교, 대청관, 문산리 5층 석탑, 신홍시장, 청백당, 문의헌, 오윤국, 이윤룡, 김유신의 유택, 경주김씨 제각, 고인돌군, 문산 사지, 문산 성지, 김옥용 효자각, 현내리, 대청리, 미천리, 광월리, 금정리의 고분군 등 18개소의 문화유적과 전통가옥 6동과 생활유물 1,200여 점을\n",
      "\n",
      "llama_print_timings:        load time =    2895.74 ms\n",
      "llama_print_timings:      sample time =      32.03 ms /   841 runs   (    0.04 ms per token, 26255.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   25400.86 ms /   841 runs   (   30.20 ms per token,    33.11 tokens per second)\n",
      "llama_print_timings:       total time =   25633.48 ms /   842 tokens\n"
     ]
    }
   ],
   "source": [
    "! ./main -m ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf --lora ../dino_2nd_LLM/training/prac/LoRA/ggml-adapter-model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/inhwancho/Desktop/dino_ai/dino_2nd_LLM/training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./prac/output/ggml-model-f32_q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = prac\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 46336\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,46336]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,46336]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,46336]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 266/46336 vs 259/46336 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 46336\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.86 B\n",
      "llm_load_print_meta: model size       = 3.64 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = prac\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   108.61 MiB, (  108.67 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3725.30 MiB\n",
      "llm_load_tensors:      Metal buffer size =   108.60 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, (  142.48 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.18 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   164.02 MiB, (  306.50 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '2048', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'prac', 'llama.vocab_size': '46336'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"./prac/output/ggml-model-f32_q4_0.gguf\"\n",
    "model = Llama(model_path = model_path,\n",
    "              n_ctx = 2048,            # context window size\n",
    "              n_gpu_layers = 1,        # enable GPU\n",
    "              use_mlock = True)        # enable memory lock so not swap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1012.99 ms\n",
      "llama_print_timings:      sample time =      13.38 ms /   120 runs   (    0.11 ms per token,  8970.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1012.95 ms /    18 tokens (   56.27 ms per token,    17.77 tokens per second)\n",
      "llama_print_timings:        eval time =    8803.01 ms /   119 runs   (   73.97 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   10090.26 ms /   137 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-7bde12b1-22d1-41b2-971d-36e4bff4d5b3',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1712637524,\n",
       " 'model': 'prac/output/ggml-model-f32_q4_0.gguf',\n",
       " 'choices': [{'text': '\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 18, 'completion_tokens': 120, 'total_tokens': 138}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "###캐모마일은 어떤 식물인가요?\n",
    "\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "output = model(prompt = prompt, max_tokens = 120, temperature = 0.2)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./main -m ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf --lora ../dino_2nd_LLM/training/prac/LoRA/ggml-adapter-model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: seed: 1712635283\n",
      "main: model base = '../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf'\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = prac\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 46336\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,46336]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,46336]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,46336]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 266/46336 vs 259/46336 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 46336\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.86 B\n",
      "llm_load_print_meta: model size       = 3.64 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = prac\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3725.30 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.18 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    98.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "main: init model\n",
      "gguf_init_from_file: invalid magic characters 'algg'\n",
      "print_params: n_vocab               : 46336\n",
      "print_params: n_ctx                 : 64\n",
      "print_params: n_embd                : 4096\n",
      "print_params: n_ff                  : 11008\n",
      "print_params: n_head                : 32\n",
      "print_params: n_head_kv             : 32\n",
      "print_params: n_layer               : 32\n",
      "print_params: norm_rms_eps          : 0.000010\n",
      "print_params: rope_freq_base        : 10000.000000\n",
      "print_params: rope_freq_scale       : 1.000000\n",
      "print_lora_params: n_rank_attention_norm : 1\n",
      "print_lora_params: n_rank_wq             : 4\n",
      "print_lora_params: n_rank_wk             : 4\n",
      "print_lora_params: n_rank_wv             : 4\n",
      "print_lora_params: n_rank_wo             : 4\n",
      "print_lora_params: n_rank_ffn_norm       : 1\n",
      "print_lora_params: n_rank_ffn_gate       : 4\n",
      "print_lora_params: n_rank_ffn_down       : 4\n",
      "print_lora_params: n_rank_ffn_up         : 4\n",
      "print_lora_params: n_rank_tok_embeddings : 4\n",
      "print_lora_params: n_rank_norm           : 1\n",
      "print_lora_params: n_rank_output         : 4\n",
      "main: total train_iterations 0\n",
      "main: seen train_samples     0\n",
      "main: seen train_tokens      0\n",
      "main: completed train_epochs 0\n",
      "main: lora_size = 85781280 bytes (81.8 MB)\n",
      "main: opt_size  = 127969264 bytes (122.0 MB)\n",
      "main: opt iter 0\n",
      "main: input_size = 47449120 bytes (45.3 MB)\n",
      "main: compute_size = 4617683296 bytes (4403.8 MB)\n",
      "main: evaluation order = RIGHT_TO_LEFT\n",
      "main: tokenize training data from /Users/inhwancho/Desktop/dino_ai/dino_2nd_LLM/Dataset/data.txt\n",
      "main: sample-start: \n",
      "main: include-sample-start: false\n",
      "tokenize_file: total number of samples: 35288403\n",
      "main: number of training tokens: 35288467\n",
      "main: number of unique tokens: 32250\n",
      "main: train data seems to have changed. restarting shuffled epoch.\n",
      "main: begin training\n",
      "main: work_size = 1112440 bytes (1.1 MB)\n",
      "train_opt_callback: iter=     0 sample=1/35288403 sched=0.000000 loss=0.000000 |->\n",
      "train_opt_callback: iter=     1 sample=5/35288403 sched=0.010000 loss=3.624161 dt=00:00:35 eta=00:17:04 |->\n",
      "train_opt_callback: iter=     2 sample=9/35288403 sched=0.020000 loss=3.480436 dt=00:00:33 eta=00:15:33 |-->\n",
      "train_opt_callback: iter=     3 sample=13/35288403 sched=0.030000 loss=3.011455 dt=00:00:32 eta=00:14:33 |------->\n",
      "train_opt_callback: iter=     4 sample=17/35288403 sched=0.040000 loss=3.611676 dt=00:00:32 eta=00:13:55 |->\n",
      "train_opt_callback: iter=     5 sample=21/35288403 sched=0.050000 loss=4.084762 dt=00:00:33 eta=00:13:57 |>\n",
      "train_opt_callback: iter=     6 sample=25/35288403 sched=0.060000 loss=4.294069 dt=00:00:33 eta=00:13:15 |>\n",
      "train_opt_callback: iter=     7 sample=29/35288403 sched=0.070000 loss=3.068207 dt=00:00:32 eta=00:12:30 |------->\n",
      "train_opt_callback: iter=     8 sample=33/35288403 sched=0.080000 loss=3.510953 dt=00:00:33 eta=00:12:24 |-->\n",
      "train_opt_callback: iter=     9 sample=37/35288403 sched=0.090000 loss=3.290525 dt=00:00:34 eta=00:12:07 |---->\n",
      "train_opt_callback: iter=    10 sample=41/35288403 sched=0.100000 loss=3.033676 dt=00:00:34 eta=00:11:26 |------->\n",
      "train_opt_callback: iter=    11 sample=45/35288403 sched=0.110000 loss=3.161297 dt=00:00:32 eta=00:10:18 |------>\n",
      "train_opt_callback: iter=    12 sample=49/35288403 sched=0.120000 loss=3.354355 dt=00:00:32 eta=00:09:38 |---->\n",
      "train_opt_callback: iter=    13 sample=53/35288403 sched=0.130000 loss=3.745145 dt=00:00:31 eta=00:09:03 |>\n",
      "train_opt_callback: iter=    14 sample=57/35288403 sched=0.140000 loss=3.391271 dt=00:00:32 eta=00:08:44 |--->\n",
      "train_opt_callback: iter=    15 sample=61/35288403 sched=0.150000 loss=3.565790 dt=00:00:34 eta=00:08:35 |-->\n",
      "train_opt_callback: iter=    16 sample=65/35288403 sched=0.160000 loss=3.874368 dt=00:00:34 eta=00:08:00 |>\n",
      "train_opt_callback: iter=    17 sample=69/35288403 sched=0.170000 loss=3.336622 dt=00:00:34 eta=00:07:25 |---->\n",
      "train_opt_callback: iter=    18 sample=73/35288403 sched=0.180000 loss=3.724122 dt=00:00:33 eta=00:06:43 |>\n",
      "train_opt_callback: iter=    19 sample=77/35288403 sched=0.190000 loss=3.404133 dt=00:00:35 eta=00:06:29 |--->\n",
      "train_opt_callback: iter=    20 sample=81/35288403 sched=0.200000 loss=3.594596 dt=00:00:33 eta=00:05:34 |->\n",
      "train_opt_callback: iter=    21 sample=85/35288403 sched=0.210000 loss=3.138273 dt=00:00:31 eta=00:04:42 |------>\n",
      "train_opt_callback: iter=    22 sample=89/35288403 sched=0.220000 loss=2.647868 dt=00:00:31 eta=00:04:13 |----------->\n",
      "train_opt_callback: iter=    23 sample=93/35288403 sched=0.230000 loss=3.919413 dt=00:00:31 eta=00:03:42 |>\n",
      "train_opt_callback: iter=    24 sample=97/35288403 sched=0.240000 loss=3.378294 dt=00:00:31 eta=00:03:07 |--->\n",
      "train_opt_callback: iter=    25 sample=101/35288403 sched=0.250000 loss=3.409724 dt=00:00:31 eta=00:02:39 |--->\n",
      "train_opt_callback: iter=    26 sample=105/35288403 sched=0.260000 loss=3.636087 dt=00:00:31 eta=00:02:07 |->\n",
      "train_opt_callback: iter=    27 sample=109/35288403 sched=0.270000 loss=3.909465 dt=00:00:31 eta=00:01:35 |>\n",
      "train_opt_callback: iter=    28 sample=113/35288403 sched=0.280000 loss=3.427457 dt=00:00:32 eta=00:01:04 |--->\n",
      "train_opt_callback: iter=    29 sample=117/35288403 sched=0.290000 loss=3.138767 dt=00:00:32 eta=00:00:32 |------>\n",
      "train_opt_callback: iter=    30 sample=121/35288403 sched=0.300000 loss=3.661369 dt=00:00:33 eta=0.0ms |->\n",
      "main: total training time: 00:17:06\n",
      "save_checkpoint_lora_file: saving to ../dino_2nd_LLM/training/prac/LoRA/checkpoint-30.gguf\n",
      "save_checkpoint_lora_file: saving to ../dino_2nd_LLM/training/prac/LoRA/checkpoint-LATEST.gguf\n",
      "save_as_llama_lora: saving to ../dino_2nd_LLM/training/prac/LoRA/ggml-lora-30-f32.gguf\n",
      "save_as_llama_lora: saving to ../dino_2nd_LLM/training/prac/LoRA/ggml-lora-LATEST-f32.gguf\n"
     ]
    }
   ],
   "source": [
    "!finetune --model-base ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf --checkpoint-in  ../dino_2nd_LLM/training/prac/LoRA/ggml-adapter-model.bin --checkpoint-out ../dino_2nd_LLM/training/prac/LoRA/checkpoint-ITERATION.gguf --lora-out ../dino_2nd_LLM/training/prac/LoRA/ggml-lora-ITERATION-f32.gguf --train-data /Users/inhwancho/Desktop/dino_ai/dino_2nd_LLM/Dataset/data.txt --save-every 300 --threads 6 --adam-iter 30 --batch 4 --ctx 64 --use-checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from prac/output/ggml-model-f32_q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = prac\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 46336\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,46336]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,46336]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,46336]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 266/46336 vs 259/46336 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 46336\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.86 B\n",
      "llm_load_print_meta: model size       = 3.64 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = prac\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   108.61 MiB, (  415.11 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3725.30 MiB\n",
      "llm_load_tensors:      Metal buffer size =   108.60 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, (  448.11 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.18 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   164.02 MiB, (  612.12 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '2048', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.block_count': '32', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'prac', 'llama.vocab_size': '46336'}\n",
      "Using fallback chat format: None\n",
      "ggml_metal_free: deallocating\n",
      "warning: failed to munlock buffer: Cannot allocate memory\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"prac/output/ggml-model-f32_q4_0.gguf\"\n",
    "model = Llama(model_path = model_path,\n",
    "              n_ctx = 2048,            # context window size\n",
    "              n_gpu_layers = 1,        # enable GPU\n",
    "              use_mlock = True)        # enable memory lock so not swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2616 (75cd4c77)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.3.9.4) for arm64-apple-darwin23.4.0\n",
      "main: seed  = 1712636494\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = prac\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 46336\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,46336]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,46336]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,46336]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 266/46336 vs 259/46336 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 46336\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.86 B\n",
      "llm_load_print_meta: model size       = 3.64 GiB (4.56 BPW) \n",
      "llm_load_print_meta: general.name     = prac\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  3623.50 MiB, ( 3623.56 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =  3623.49 MiB\n",
      "llm_load_tensors:        CPU buffer size =   101.81 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/inhwancho/Desktop/dino_ai/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   256.00 MiB, ( 3881.38 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.18 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    98.50 MiB, ( 3979.88 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =    98.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "llama_apply_lora_from_file_internal: applying lora adapter from '../dino_2nd_LLM/training/prac/LoRA/ggml-lora-LATEST-f32.gguf' - please wait ...\n",
      "llama_apply_lora_from_file_internal: r = 4, alpha = 4, scaling = 1.00\n",
      "llama_apply_lora_from_file_internal: warning: using a lora adapter with a quantized model may result in poor quality, use a f16 or f32 base model with --lora-base\n",
      "........................................................................ done (13232.97 ms)\n",
      "\n",
      "system_info: n_threads = 6 / 8 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "\n",
      " 상징의 발견』의 출간을 기념하여 이 책의 저자이자 번역자인 박연미 기자를 만나 인터뷰하였다. ​​1. 『상징의 발견』을 쓰게 된 배경​상징은 인간의 가장 원초적인 언어이며, 인간이 문명을 만드는 데 가장 필수적인 요소입니다. 인간은 상징을 통해 다른 인간과 소통을 하고, 언어를 만들고, 문자를 만들어냅니다. 따라서 인간은 상징을 통해 과거를 기억하고 현재를 이해하며, 미래를 그려볼 수 있게 됩니다. ​ 『상징의 발견』은 인간과 문명의 탄생과 함께해 온 상징의 역사를 낱낱이 해부하며, 인간의 삶과 문명이 상징을 통해 어떻게 발전하고 변화했는지를 살펴봅니다. 상징에 대한 기존의 연구는 주로 문학을 중심으로 이뤄져 왔지만, 이 책은 문학뿐만 아니라 철학, 미술, 음악, 건축, 심리학, 인류학 등 다양한 학문을 넘나들며 상징의 역사를 소개합니다. ​ 2. 상징의 4가지 구성​상징은 크게 4가지로 구성된다. ​첫째, 상징의 기원은 문명의 기원이다. ​상징의 기원은 문명의 기원이다. 상징은 문명이라는 거대한 구조물이 만들어지기 전에는 존재하지 않았다. 따라서 상징은 문명이라는 구조물의 일부이다. 상징은 문명이라는 구조물에서 떨어져나와 홀로 존재하는 것이 아니다.\n",
      "\n",
      "llama_print_timings:        load time =   15458.66 ms\n",
      "llama_print_timings:      sample time =      10.65 ms /   301 runs   (    0.04 ms per token, 28260.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    9423.11 ms /   301 runs   (   31.31 ms per token,    31.94 tokens per second)\n",
      "llama_print_timings:       total time =    9535.56 ms /   302 tokens\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "!main -m ../dino_2nd_LLM/training/prac/output/ggml-model-f32_q4_0.gguf --lora ../dino_2nd_LLM/training/prac/LoRA/ggml-lora-LATEST-f32.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
